{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic and Qrel System Ordering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters configuration\n",
    "task = 'fullrank'\n",
    "metric = 'ndcg_cut_10'\n",
    "cut_off = '10'\n",
    "\n",
    "# File path\n",
    "metadata_file = 'metadata'\n",
    "\n",
    "nist_qrel_file = 'qrels/2023.qrels.pass.withDupes.txt'\n",
    "\n",
    "nist_ndcgeval_file = 'results/TRECDL2023/all.pass.nist.ndcgeval'\n",
    "gpt4_ndcgeval_file = 'results/TRECDL2023/all.pass.gpt4.ndcgeval'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading metadata\n",
    "Loading the metadata file to create a dictionary of runid to subtask, loading the metadata_models file to have the pipeline structure, adding the subtask info to metadata models dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runid_to_subtasks = dict()\n",
    "with open(metadata_file, 'rt', encoding = 'utf8') as f:\n",
    "    reader = csv.reader(f, delimiter = ':')\n",
    "    for [runid, group, _, _, _, _, maintask, _, subtask, _, _, _, _, model, _, _, _, _] in reader:\n",
    "        runid_to_subtasks[runid] = subtask.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading metadeta infomrmaiton for runs, we have the type of pipeline as a metadata here\n",
    "metadata_models_df = pd.read_csv('metadata_models.csv', sep=\"\\t\")\n",
    "metadata_models_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_models_df['subtask'] = metadata_models_df['run_id'].map(runid_to_subtasks)\n",
    "metadata_models_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading System Performance based on NIST Qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_qrels = pd.read_csv(nist_qrel_file, sep=' ', header=None, names=['qid', 'Q0', 'docid', 'rel'])\n",
    "\n",
    "queries_judged = set(nist_qrels['qid'])\n",
    "\n",
    "real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "gpt4_queries_judged = [x for x in queries_judged if x > 3100000]\n",
    "\n",
    "print('queries_judged', len(queries_judged))\n",
    "print('real_queries_judged', len(real_queries_judged))\n",
    "print('t5_queries_judged', len(t5_queries_judged))\n",
    "print('gpt4_queries_judged', len(gpt4_queries_judged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading runs ndcgecal on NIST qrels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_labels_ndcgeval = pd.read_csv(nist_ndcgeval_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "# nist_labels_ndcgeval['runid_metric'] = nist_labels_ndcgeval['runid_metric'].str.strip()\n",
    "# nist_labels_ndcgeval[['run_id', 'metric']] = nist_labels_ndcgeval['runid_metric'].str.split(' ', expand=True)\n",
    "# nist_labels_ndcgeval = nist_labels_ndcgeval.drop('runid_metric', axis=1)\n",
    "nist_labels_ndcgeval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the runs that are for reranking task (4 runs for reranking) -> 39 runs will be remain\n",
    "included_runids = metadata_models_df[metadata_models_df['subtask'] == task]['run_id']\n",
    "print(len(set(included_runids)))\n",
    "\n",
    "# We have {'D_bm25_splades', 'D_naverloo-frgpt4', 'D_naverloo_bm25_RR', 'D_naverloo_bm_splade_RR', 'colbertv2'}\n",
    "# runs that have not submitted their results. So, we have 40 run_ids in metadata file in total, the rerank ones (4 runs) \n",
    "# are rmeoved from the list using the above command. Then, 36 runs we have, in whiche 5 submissions are have not submitteed their reuslt\n",
    "# so we will have 31 submissions in total.\n",
    "\n",
    "# nist_labels_ndcgeval = nist_labels_ndcgeval[nist_labels_ndcgeval['run_id'].isin(included_runids)]\n",
    "print(len(set(nist_labels_ndcgeval['run_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_labels_ndcgeval = nist_labels_ndcgeval[nist_labels_ndcgeval['qid'] != 'all']\n",
    "nist_labels_ndcgeval['qid'] = nist_labels_ndcgeval['qid'].astype(int)\n",
    "nist_labels_ndcgeval['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(nist_labels_ndcgeval['run_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_labels_ndcgeval = nist_labels_ndcgeval[nist_labels_ndcgeval['qid'] != 'all']\n",
    "nist_labels_ndcgeval['qid'] = nist_labels_ndcgeval['qid'].astype(int)\n",
    "\n",
    "track_queries_nist_labels = nist_labels_ndcgeval[(nist_labels_ndcgeval['metric'] == metric) & (nist_labels_ndcgeval['qid'].isin(queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "track_queries_nist_labels = track_queries_nist_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_track_nist = dict(zip(track_queries_nist_labels['run_id'], track_queries_nist_labels[metric]))\n",
    "\n",
    "generated_queries_nist_labels = nist_labels_ndcgeval[(nist_labels_ndcgeval['metric'] == metric) & ((nist_labels_ndcgeval['qid'].isin(t5_queries_judged)) | ((nist_labels_ndcgeval['qid'].isin(gpt4_queries_judged)))) ].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "generated_queries_nist_labels = generated_queries_nist_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_generated_nist = dict(zip(generated_queries_nist_labels['run_id'], generated_queries_nist_labels[metric]))\n",
    "\n",
    "real_queries_nist_labels = nist_labels_ndcgeval[(nist_labels_ndcgeval['metric'] == metric) & (nist_labels_ndcgeval['qid'].isin(real_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "real_queries_nist_labels = real_queries_nist_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_real_nist = dict(zip(real_queries_nist_labels['run_id'], real_queries_nist_labels[metric]))\n",
    "\n",
    "t5_queries_nist_labels = nist_labels_ndcgeval[(nist_labels_ndcgeval['metric'] == metric) & (nist_labels_ndcgeval['qid'].isin(t5_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "t5_queries_nist_labels = t5_queries_nist_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_t5_nist = dict(zip(t5_queries_nist_labels['run_id'], t5_queries_nist_labels[metric]))\n",
    "\n",
    "gpt4_queries_nist_labels = nist_labels_ndcgeval[(nist_labels_ndcgeval['metric'] == metric) & (nist_labels_ndcgeval['qid'].isin(gpt4_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "gpt4_queries_nist_labels = gpt4_queries_nist_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_gpt4_nist = dict(zip(gpt4_queries_nist_labels['run_id'], gpt4_queries_nist_labels[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading runs ndcgecal on Synthetic (GPT-4 -- V3) qrels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_labels_ndcgeval = pd.read_csv(gpt4_ndcgeval_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "gpt4_labels_ndcgeval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4_labels_ndcgeval = gpt4_labels_ndcgeval[gpt4_labels_ndcgeval['run_id'].isin(included_runids)]\n",
    "len(set(gpt4_labels_ndcgeval['run_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_labels_ndcgeval = gpt4_labels_ndcgeval[gpt4_labels_ndcgeval['qid'] != 'all']\n",
    "gpt4_labels_ndcgeval['qid'] = gpt4_labels_ndcgeval['qid'].astype(int)\n",
    "\n",
    "track_queries_gpt4_labels = gpt4_labels_ndcgeval[(gpt4_labels_ndcgeval['metric'] == metric) & (gpt4_labels_ndcgeval['qid'].isin(queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "track_queries_gpt4_labels = track_queries_gpt4_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_track_gpt4 = dict(zip(track_queries_gpt4_labels['run_id'], track_queries_gpt4_labels[metric]))\n",
    "\n",
    "real_queries_gpt4_labels = gpt4_labels_ndcgeval[(gpt4_labels_ndcgeval['metric'] == metric) & (gpt4_labels_ndcgeval['qid'].isin(real_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "real_queries_gpt4_labels = real_queries_gpt4_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_real_gpt4 = dict(zip(real_queries_gpt4_labels['run_id'], real_queries_gpt4_labels[metric]))\n",
    "\n",
    "generated_queries_gpt4_labels = gpt4_labels_ndcgeval[(gpt4_labels_ndcgeval['metric'] == metric) & ((gpt4_labels_ndcgeval['qid'].isin(t5_queries_judged)) | ((gpt4_labels_ndcgeval['qid'].isin(gpt4_queries_judged)))) ].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "generated_queries_gpt4_labels = generated_queries_gpt4_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_generated_gpt4 = dict(zip(generated_queries_gpt4_labels['run_id'], generated_queries_gpt4_labels[metric]))\n",
    "\n",
    "t5_queries_gpt4_labels = gpt4_labels_ndcgeval[(gpt4_labels_ndcgeval['metric'] == metric) & (gpt4_labels_ndcgeval['qid'].isin(t5_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "t5_queries_gpt4_labels = t5_queries_gpt4_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_t5_gpt4 = dict(zip(t5_queries_gpt4_labels['run_id'], t5_queries_gpt4_labels[metric]))\n",
    "\n",
    "gpt4_queries_gpt4_labels = gpt4_labels_ndcgeval[(gpt4_labels_ndcgeval['metric'] == metric) & (gpt4_labels_ndcgeval['qid'].isin(gpt4_queries_judged))].groupby('run_id')['score'].agg(['mean', 'count']).reset_index()\n",
    "gpt4_queries_gpt4_labels = gpt4_queries_gpt4_labels.rename(columns={'mean':metric})\n",
    "runid_to_score_gpt4_gpt4 = dict(zip(gpt4_queries_gpt4_labels['run_id'], gpt4_queries_gpt4_labels[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the metadata and assigning LLM tag based on the pipeline of each run\n",
    "run_to_model = {}\n",
    "# red: gpt, yellow: t5, orange: gpt+t5, and blue: everything else\n",
    "for eachline in metadata_models_df.itertuples(index=True):\n",
    "    # if we have both T5 and GPT version in the pipeline\n",
    "    if 'T5' in eachline.LLM and 'GPT' in eachline.LLM:\n",
    "        run_to_model[eachline.run_id] = 'GPT+T5'\n",
    "    elif 'GPT' in eachline.LLM:\n",
    "        run_to_model[eachline.run_id] = 'GPT'\n",
    "    elif 'T5' in eachline.LLM:\n",
    "        run_to_model[eachline.run_id] = 'T5'\n",
    "    else:\n",
    "        run_to_model[eachline.run_id] = 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_df(dict_1, dict_2):\n",
    "\n",
    "    # Convert dictionaries to DataFrames\n",
    "    df1 = pd.DataFrame(list(dict_1.items()), columns=['Key', 'Value1'])\n",
    "    df2 = pd.DataFrame(list(dict_2.items()), columns=['Key', 'Value2'])\n",
    "\n",
    "    # Merge the DataFrames based on the 'Key' column\n",
    "    merged_df = pd.merge(df1, df2, on='Key', how='outer')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_color = {'GPT': 'red', 'T5': 'brown', 'GPT+T5': 'orange', 'Others': 'green'}\n",
    "model_to_marker = {'GPT': 'x', 'T5': '|', 'GPT+T5': '+', 'Others': '.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Plots for Various Types of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(x_avg_scores, y_avg_scores, query_types=['track', 'real'], label_types=['NIST', 'NIST']):\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.set_style(\"ticks\")  # Options: white, dark, whitegrid, darkgrid, ticks\n",
    "    sns.set_context(\"poster\", font_scale =0.6)     # Options: paper, notebook, talk, poster\n",
    "    sns.set_palette(\"bright\")   # You can also use: deep, muted, bright, dark, colorblind, or a custom list of colors\n",
    "\n",
    "    # splitting query types\n",
    "    query_type_x = query_types[0]\n",
    "    query_type_y = query_types[1]\n",
    "\n",
    "    # splitting query types\n",
    "    label_type_x = label_types[0]\n",
    "    label_type_y = label_types[1]\n",
    "\n",
    "    if label_type_x == 'Sparse':\n",
    "        metric_type_x = f\"NDCG@{cut_off}\"\n",
    "    else:\n",
    "        metric_type_x = f\"NDCG@{cut_off}\"\n",
    "\n",
    "    if label_type_y == 'Sparse':\n",
    "        metric_type_y = f\"NDCG@{cut_off}\"\n",
    "    else:\n",
    "        metric_type_y = f\"NDCG@{cut_off}\"\n",
    "\n",
    "    merged_df = get_merged_df(x_avg_scores, y_avg_scores)\n",
    "\n",
    "    for eachline in merged_df.itertuples(index=True):\n",
    "        run_model = run_to_model[eachline.Key]\n",
    "        model_color = model_to_color[run_model]\n",
    "        model_marker = model_to_marker[run_model]\n",
    "        plt.scatter(eachline.Value1, eachline.Value2, c=f\"{model_color}\", marker=f\"{model_marker}\", label=f\"{run_model}\", s=90)\n",
    "\n",
    "    if label_type_x != 'Sparse' and label_type_y != 'Sparse':\n",
    "        # axes\n",
    "        ax = plt.gca()\n",
    "        lims = [\n",
    "            np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "            np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "            ]\n",
    "        \n",
    "        # now plot both limits against eachother\n",
    "        ax.plot(lims, lims, color='blue', linestyle='dashed', linewidth=0.7)\n",
    "    \n",
    "    tau, p_value = stats.kendalltau(merged_df['Value1'], merged_df['Value2'])\n",
    "\n",
    "    #adding tau inside the plot \n",
    "    plt.annotate(r'Kendall $\\tau$ = %s' %np.round(tau, 4), xy=(0.03, 0.55), xycoords='axes fraction', fontsize=16)\n",
    "    \n",
    "    # plt.xlabel(f'{metric_type_x} on {query_type_x} queries ({label_type_x} Qrel)', fontsize=10)\n",
    "    # plt.ylabel(f'{metric_type_y} on {query_type_y} queries ({label_type_y} Qrel)', fontsize=10)\n",
    "\n",
    "    plt.xlabel(f'{query_type_x} Queries, {label_type_x} Labels', fontsize=18)\n",
    "    plt.ylabel(f'{query_type_y} Queries, {label_type_y} Labels', fontsize=18)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), edgecolor='black', fontsize=16)\n",
    "\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    plt.savefig(f\"figs/system-ranking/{query_type_x}-{label_type_x}_{query_type_y}-{label_type_y}_plot.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generating Query Analysis Plots (Synthetic Query Analysis -- NIST Qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(runid_to_score_real_nist, runid_to_score_t5_nist, query_types=['Real', 'T5'], label_types=['Human', 'Human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real vs. track\n",
    "get_plot(runid_to_score_real_nist, runid_to_score_real_gpt4, query_types=['Real', 'Real'], label_types=['Human', 'GPT-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real vs. track\n",
    "get_plot(runid_to_score_real_nist, runid_to_score_generated_nist, query_types=['Real', 'Synthetic'], label_types=['Human', 'Human'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
