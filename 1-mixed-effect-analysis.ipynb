{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"ndcg_cut_10\" # map # ndcg_cut_10\n",
    "result_format = \"ndcgeval\" # treceval # ndcgeval\n",
    "results_from = \"LLMJudge2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'./results/{results_from}/*.{result_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./results/LLMJudge2024/all.pass.TREMA-rubric0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.RMITIR-llama70B.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-thomas3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-simple3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.NISTRetrieval-instruct0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-test.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-direct.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-gpt4o.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-4prompts.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-exp.ndcgeval']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for removed_labler in ['RMITIR-llama70B', 'llmjudge-simple3', 'Olz-exp', 'TREMA-4prompts', 'TREMA-direct']:\n",
    "    removed_labler = f'./results/LLMJudge2024/all.pass.{removed_labler}.{result_format}'\n",
    "    files.remove(removed_labler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>score</th>\n",
       "      <th>judged_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2002168</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004282</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004980</td>\n",
       "      <td>0.4221</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2005952</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2007816</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100119</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9052</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100235</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100289</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100399</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100825</td>\n",
       "      <td>0.6347</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4375 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_id      qid   score      judged_by\n",
       "2           naverloo_fs_RR_duo  2002168  1.0000  TREMA-rubric0\n",
       "12          naverloo_fs_RR_duo  2004282  0.3396  TREMA-rubric0\n",
       "22          naverloo_fs_RR_duo  2004980  0.4221  TREMA-rubric0\n",
       "32          naverloo_fs_RR_duo  2005952  0.6944  TREMA-rubric0\n",
       "42          naverloo_fs_RR_duo  2007816  0.6512  TREMA-rubric0\n",
       "...                        ...      ...     ...            ...\n",
       "9042  naverloo_bm25_splades_RR  3100119  0.9558      Olz-gpt4o\n",
       "9052  naverloo_bm25_splades_RR  3100235  0.6795      Olz-gpt4o\n",
       "9062  naverloo_bm25_splades_RR  3100289  0.6726      Olz-gpt4o\n",
       "9072  naverloo_bm25_splades_RR  3100399  0.4828      Olz-gpt4o\n",
       "9082  naverloo_bm25_splades_RR  3100825  0.6347      Olz-gpt4o\n",
       "\n",
       "[4375 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_list = []\n",
    "\n",
    "for infile in files:\n",
    "    judger = infile.split('/')[3].split('.')[2]\n",
    "    result_df = pd.read_csv(infile, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    results_df_list.append(result_df)\n",
    " \n",
    "results_dfs = pd.concat(results_df_list)\n",
    "results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_judged = set(results_dfs['qid'])\n",
    "# real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "# t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "# gpt4_queries_judged = [x for x in queries_judged if x > 3100000]\n",
    "\n",
    "# print(len(real_queries_judged))\n",
    "# print(len(t5_queries_judged))\n",
    "# print(len(gpt4_queries_judged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/doc_to_info.txt\", sep='\\t')\n",
    "model_to_info = pd.read_csv(\"infos/model_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(results_dfs, qid_to_info, on='qid')\n",
    "# data = pd.merge(data, doc_to_info, on='qid')\n",
    "data = pd.merge(data, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features that do not seem to be very useful, relevant, or concrete.\n",
    "# QL is a binary feature and we have QW, so it should be dropped.\n",
    "data.drop(['QL'], axis=1, inplace=True)\n",
    "data.drop(['isGPT4'], axis=1, inplace=True)\n",
    "data.drop(['QDR'], axis=1, inplace=True)\n",
    "data.drop(['QDS'], axis=1, inplace=True)\n",
    "data.drop(['isLLM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT'] = data['QT'].astype('category') \n",
    "data['QT'] = data['QT'].replace({0: 'Human', 1: 'T5', 2: 'GPT4'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeler: {'TREMA-rubric0', 'RMITIR-llama70B', 'llmjudge-thomas3', 'llmjudge-simple3', 'NISTRetrieval-instruct0', 'llmjudge-test', \n",
    "# 'TREMA-direct', 'Olz-gpt4o', 'TREMA-4prompts', 'Olz-exp'}\n",
    "\n",
    "data['judged_by'] = data['judged_by'].replace({'TREMA-rubric0': 'Llama3Rubric', 'RMITIR-llama70B': 'Llama3RMIT', 'llmjudge-thomas3': 'GPT4Thomas', 'llmjudge-simple3': 'GPT4Simple', \n",
    "                                               'NISTRetrieval-instruct0': 'Llama3Inst', 'llmjudge-test': 'NIST', 'TREMA-direct': 'FlanT5Direct', 'Olz-gpt4o': 'GPT4oSimple',\n",
    "                                               'TREMA-4prompts': 'Llama3Prompts', 'Olz-exp': 'GPT4oExp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model = \"score ~ QW + APL + MN + C(judged_by, Treatment(reference='NIST')) +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) + C(ST, Treatment(reference='Other')) * C(judged_by, Treatment(reference='NIST'))  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>score</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>  <td>4375</td>         <td>Method:</td>         <td>REML</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>25</td>          <td>Scale:</td>         <td>0.0362</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>    <td>175</td>     <td>Log-Likelihood:</td>   <td>904.5770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>    <td>175</td>       <td>Converged:</td>         <td>Yes</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>   <td>175.0</td>           <td></td>               <td></td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                                     <td></td>                                                      <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                                                                <td>0.426</td>   <td>0.104</td>   <td>4.076</td> <td>0.000</td>  <td>0.221</td>  <td>0.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>                                                  <td>0.181</td>   <td>0.015</td>  <td>12.142</td> <td>0.000</td>  <td>0.152</td>  <td>0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>                                                 <td>0.098</td>   <td>0.015</td>   <td>6.576</td> <td>0.000</td>  <td>0.069</td>  <td>0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>                                                  <td>0.210</td>   <td>0.015</td>  <td>14.066</td> <td>0.000</td>  <td>0.181</td>  <td>0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>                                               <td>-0.027</td>   <td>0.015</td>  <td>-1.833</td> <td>0.067</td> <td>-0.057</td>  <td>0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]</th>                                                                                                <td>-0.088</td>   <td>0.079</td>  <td>-1.115</td> <td>0.265</td> <td>-0.242</td>  <td>0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]</th>                                                                                               <td>0.079</td>   <td>0.104</td>   <td>0.758</td> <td>0.449</td> <td>-0.125</td>  <td>0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                              <td>-0.054</td>   <td>0.020</td>  <td>-2.716</td> <td>0.007</td> <td>-0.094</td> <td>-0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]</th>                                                                <td>0.024</td>   <td>0.019</td>   <td>1.233</td> <td>0.218</td> <td>-0.014</td>  <td>0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                            <td>0.174</td>   <td>0.018</td>   <td>9.544</td> <td>0.000</td>  <td>0.139</td>  <td>0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                     <td>-0.055</td>   <td>0.022</td>  <td>-2.536</td> <td>0.011</td> <td>-0.097</td> <td>-0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                   <td>-0.023</td>   <td>0.022</td>  <td>-1.052</td> <td>0.293</td> <td>-0.065</td>  <td>0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                       <td>0.014</td>   <td>0.020</td>   <td>0.730</td> <td>0.465</td> <td>-0.024</td>  <td>0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                    <td>-0.038</td>   <td>0.020</td>  <td>-1.908</td> <td>0.056</td> <td>-0.076</td>  <td>0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                  <td>-0.023</td>   <td>0.020</td>  <td>-1.182</td> <td>0.237</td> <td>-0.062</td>  <td>0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                <td>-0.028</td>   <td>0.020</td>  <td>-1.420</td> <td>0.156</td> <td>-0.067</td>  <td>0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>      <td>-0.011</td>   <td>0.027</td>  <td>-0.431</td> <td>0.667</td> <td>-0.063</td>  <td>0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>        <td>0.012</td>   <td>0.024</td>   <td>0.498</td> <td>0.618</td> <td>-0.035</td>  <td>0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>    <td>0.032</td>   <td>0.024</td>   <td>1.342</td> <td>0.180</td> <td>-0.015</td>  <td>0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>      <td>0.003</td>   <td>0.027</td>   <td>0.115</td> <td>0.909</td> <td>-0.049</td>  <td>0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>       <td>0.019</td>   <td>0.024</td>   <td>0.797</td> <td>0.426</td> <td>-0.028</td>  <td>0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>   <td>0.055</td>   <td>0.024</td>   <td>2.280</td> <td>0.023</td>  <td>0.008</td>  <td>0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>       <td>0.002</td>   <td>0.027</td>   <td>0.058</td> <td>0.954</td> <td>-0.051</td>  <td>0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>       <td>-0.005</td>   <td>0.024</td>  <td>-0.206</td> <td>0.837</td> <td>-0.052</td>  <td>0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>   <td>-0.014</td>   <td>0.024</td>  <td>-0.586</td> <td>0.558</td> <td>-0.062</td>  <td>0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>     <td>0.001</td>   <td>0.027</td>   <td>0.041</td> <td>0.967</td> <td>-0.051</td>  <td>0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>     <td>-0.047</td>   <td>0.024</td>  <td>-1.948</td> <td>0.051</td> <td>-0.095</td>  <td>0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th> <td>-0.061</td>   <td>0.024</td>  <td>-2.511</td> <td>0.012</td> <td>-0.108</td> <td>-0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QW</th>                                                                                                      <td>-0.007</td>   <td>0.016</td>  <td>-0.422</td> <td>0.673</td> <td>-0.038</td>  <td>0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>APL</th>                                                                                                     <td>-0.000</td>   <td>0.000</td>  <td>-0.350</td> <td>0.726</td> <td>-0.000</td>  <td>0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MN</th>                                                                                                       <td>0.022</td>   <td>0.002</td>  <td>12.955</td> <td>0.000</td>  <td>0.019</td>  <td>0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group Var</th>                                                                                                <td>0.022</td>   <td>0.036</td>     <td></td>      <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table><br/>\n"
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Mixed Linear Model Regression Results}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:            & MixedLM & Dependent Variable: & score     \\\\\n",
       "No. Observations: & 4375    & Method:             & REML      \\\\\n",
       "No. Groups:       & 25      & Scale:              & 0.0362    \\\\\n",
       "Min. group size:  & 175     & Log-Likelihood:     & 904.5770  \\\\\n",
       "Max. group size:  & 175     & Converged:          & Yes       \\\\\n",
       "Mean group size:  & 175.0   &                     &           \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                                                                                                         &  Coef. & Std.Err. &      z & P$> |$z$|$ & [0.025 & 0.975]  \\\\\n",
       "\\hline\n",
       "Intercept                                                                                                &  0.426 &    0.104 &  4.076 &       0.000 &  0.221 &  0.631  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]                                                 &  0.181 &    0.015 & 12.142 &       0.000 &  0.152 &  0.210  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                &  0.098 &    0.015 &  6.576 &       0.000 &  0.069 &  0.127  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]                                                 &  0.210 &    0.015 & 14.066 &       0.000 &  0.181 &  0.239  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]                                               & -0.027 &    0.015 & -1.833 &       0.067 & -0.057 &  0.002  \\\\\n",
       "QT[T.T5]                                                                                                 & -0.088 &    0.079 & -1.115 &       0.265 & -0.242 &  0.066  \\\\\n",
       "QT[T.GPT4]                                                                                               &  0.079 &    0.104 &  0.758 &       0.449 & -0.125 &  0.282  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                               & -0.054 &    0.020 & -2.716 &       0.007 & -0.094 & -0.015  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                &  0.024 &    0.019 &  1.233 &       0.218 & -0.014 &  0.062  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                            &  0.174 &    0.018 &  9.544 &       0.000 &  0.139 &  0.210  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                      & -0.055 &    0.022 & -2.536 &       0.011 & -0.097 & -0.012  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                    & -0.023 &    0.022 & -1.052 &       0.293 & -0.065 &  0.020  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                       &  0.014 &    0.020 &  0.730 &       0.465 & -0.024 &  0.053  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                     & -0.038 &    0.020 & -1.908 &       0.056 & -0.076 &  0.001  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                   & -0.023 &    0.020 & -1.182 &       0.237 & -0.062 &  0.015  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                 & -0.028 &    0.020 & -1.420 &       0.156 & -0.067 &  0.011  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]      & -0.011 &    0.027 & -0.431 &       0.667 & -0.063 &  0.041  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]       &  0.012 &    0.024 &  0.498 &       0.618 & -0.035 &  0.059  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]   &  0.032 &    0.024 &  1.342 &       0.180 & -0.015 &  0.080  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]     &  0.003 &    0.027 &  0.115 &       0.909 & -0.049 &  0.055  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]      &  0.019 &    0.024 &  0.797 &       0.426 & -0.028 &  0.067  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]  &  0.055 &    0.024 &  2.280 &       0.023 &  0.008 &  0.103  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]      &  0.002 &    0.027 &  0.058 &       0.954 & -0.051 &  0.054  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]       & -0.005 &    0.024 & -0.206 &       0.837 & -0.052 &  0.042  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]   & -0.014 &    0.024 & -0.586 &       0.558 & -0.062 &  0.033  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]    &  0.001 &    0.027 &  0.041 &       0.967 & -0.051 &  0.053  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]     & -0.047 &    0.024 & -1.948 &       0.051 & -0.095 &  0.000  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric] & -0.061 &    0.024 & -2.511 &       0.012 & -0.108 & -0.013  \\\\\n",
       "QW                                                                                                       & -0.007 &    0.016 & -0.422 &       0.673 & -0.038 &  0.024  \\\\\n",
       "APL                                                                                                      & -0.000 &    0.000 & -0.350 &       0.726 & -0.000 &  0.000  \\\\\n",
       "MN                                                                                                       &  0.022 &    0.002 & 12.955 &       0.000 &  0.019 &  0.025  \\\\\n",
       "Group Var                                                                                                &  0.022 &    0.036 &        &             &        &         \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                                      Mixed Linear Model Regression Results\n",
       "==================================================================================================================================================\n",
       "Model:                                            MixedLM                               Dependent Variable:                               score   \n",
       "No. Observations:                                 4375                                  Method:                                           REML    \n",
       "No. Groups:                                       25                                    Scale:                                            0.0362  \n",
       "Min. group size:                                  175                                   Log-Likelihood:                                   904.5770\n",
       "Max. group size:                                  175                                   Converged:                                        Yes     \n",
       "Mean group size:                                  175.0                                                                                           \n",
       "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "                                                                                                        Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
       "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                                                                0.426    0.104  4.076 0.000  0.221  0.631\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]                                                  0.181    0.015 12.142 0.000  0.152  0.210\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                 0.098    0.015  6.576 0.000  0.069  0.127\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]                                                  0.210    0.015 14.066 0.000  0.181  0.239\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]                                               -0.027    0.015 -1.833 0.067 -0.057  0.002\n",
       "QT[T.T5]                                                                                                -0.088    0.079 -1.115 0.265 -0.242  0.066\n",
       "QT[T.GPT4]                                                                                               0.079    0.104  0.758 0.449 -0.125  0.282\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                              -0.054    0.020 -2.716 0.007 -0.094 -0.015\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                0.024    0.019  1.233 0.218 -0.014  0.062\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                            0.174    0.018  9.544 0.000  0.139  0.210\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                     -0.055    0.022 -2.536 0.011 -0.097 -0.012\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                   -0.023    0.022 -1.052 0.293 -0.065  0.020\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                       0.014    0.020  0.730 0.465 -0.024  0.053\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                    -0.038    0.020 -1.908 0.056 -0.076  0.001\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                  -0.023    0.020 -1.182 0.237 -0.062  0.015\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                -0.028    0.020 -1.420 0.156 -0.067  0.011\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]      -0.011    0.027 -0.431 0.667 -0.063  0.041\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]        0.012    0.024  0.498 0.618 -0.035  0.059\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]    0.032    0.024  1.342 0.180 -0.015  0.080\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]      0.003    0.027  0.115 0.909 -0.049  0.055\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]       0.019    0.024  0.797 0.426 -0.028  0.067\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]   0.055    0.024  2.280 0.023  0.008  0.103\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]       0.002    0.027  0.058 0.954 -0.051  0.054\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]       -0.005    0.024 -0.206 0.837 -0.052  0.042\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]   -0.014    0.024 -0.586 0.558 -0.062  0.033\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]     0.001    0.027  0.041 0.967 -0.051  0.053\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]     -0.047    0.024 -1.948 0.051 -0.095  0.000\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric] -0.061    0.024 -2.511 0.012 -0.108 -0.013\n",
       "QW                                                                                                      -0.007    0.016 -0.422 0.673 -0.038  0.024\n",
       "APL                                                                                                     -0.000    0.000 -0.350 0.726 -0.000  0.000\n",
       "MN                                                                                                       0.022    0.002 12.955 0.000  0.019  0.025\n",
       "Group Var                                                                                                0.022    0.036                           \n",
       "==================================================================================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model, data, groups=data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['QD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_data = data2.pivot(index=['run_id', 'qid', 'QW', 'APL', 'QT', 'ST', 'pipeline'], columns=['judged_by'], values='score')\n",
    "score_diff_data.columns.name = None\n",
    "score_diff_data = score_diff_data.reset_index()\n",
    "score_diff_data['score_diff'] = score_diff_data['gpt4'] - score_diff_data['nist']\n",
    "score_diff_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_mixed_model = \"score_diff ~  QW + APL + pipeline  +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(score_diff_mixed_model, score_diff_data, groups=score_diff_data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_condition_Qreal = \"score ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\"\n",
    "mixed_model_condition_Qsynthetic = \"score ~ QL + QDS + QW + C(LLM, Treatment(reference='Other')) + pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_real_judgments, groups=real_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_real_judgments, groups=synthetic_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_synthetic_judgments, groups=real_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_synthetic_judgments, groups=synthetic_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "real_queries_diff = pd.merge(real_queries_real_judgments, real_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "real_queries_diff['score_ab'] = real_queries_diff['score_a'] - real_queries_diff['score_b']\n",
    "real_queries_diff['score_ba'] = real_queries_diff['score_b'] - real_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "synthetic_queries_diff = pd.merge(synthetic_queries_real_judgments, synthetic_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "synthetic_queries_diff['score_ab'] = synthetic_queries_diff['score_a'] - synthetic_queries_diff['score_b']\n",
    "synthetic_queries_diff['score_ba'] = synthetic_queries_diff['score_b'] - synthetic_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments (refer to: \"Extra Exp. 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_file):\n",
    "    result_df = pd.read_csv(result_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_judge_results = get_result(result_file=\"results/all.pass.nist.ndcgeval\")\n",
    "synthetic_judge_results = get_result(result_file=\"results/all.pass.gpt4.ndcgeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "results_diff = pd.merge(real_judge_results, synthetic_judge_results, on=['run_id', 'qid'], suffixes=('_real', '_synthetic'))\n",
    "# Subtracting the 'Score' values\n",
    "results_diff['score_RS'] = results_diff['score_real'] - results_diff['score_synthetic']\n",
    "results_diff['score_SR'] = results_diff['score_synthetic'] - results_diff['score_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff = pd.merge(results_diff, qid_to_info, on='qid')\n",
    "results_diff = pd.merge(results_diff, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected: score_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff['qd_RS'] = results_diff['QDR'] - results_diff['QDS']\n",
    "results_diff['qd_SR'] = results_diff['QDS'] - results_diff['QDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_1 = \"score_SR ~ Synthetic + QL + qd_SR + QW + DL + pipeline + C(LLM, Treatment(reference='Other')) + Synthetic * C(LLM, Treatment(reference='Other'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_1, results_diff, groups=results_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
