{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"ndcg_cut_10\" # map # ndcg_cut_10\n",
    "result_format = \"ndcgeval\" # treceval # ndcgeval\n",
    "results_from = \"LLMJudge2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'./results/{results_from}/*.{result_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./results/LLMJudge2024/all.pass.TREMA-rubric0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.RMITIR-llama70B.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-thomas3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-simple3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.NISTRetrieval-instruct0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-test.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-direct.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-gpt4o.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-4prompts.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-exp.ndcgeval']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for removed_labler in ['RMITIR-llama70B', 'llmjudge-simple3', 'Olz-exp', 'TREMA-4prompts', 'TREMA-direct']:\n",
    "#     removed_labler = f'./results/LLMJudge2024/all.pass.{removed_labler}.{result_format}'\n",
    "#     files.remove(removed_labler)\n",
    "\n",
    "for removed_labler in ['TREMA-rubric0', 'RMITIR-llama70B', 'llmjudge-thomas3', 'Olz-exp', 'llmjudge-simple3', 'NISTRetrieval-instruct0', 'TREMA-4prompts']:\n",
    "    removed_labler = f'./results/LLMJudge2024/all.pass.{removed_labler}.{result_format}'\n",
    "    files.remove(removed_labler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>score</th>\n",
       "      <th>judged_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2002168</td>\n",
       "      <td>0.6996</td>\n",
       "      <td>llmjudge-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004282</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>llmjudge-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004980</td>\n",
       "      <td>0.3832</td>\n",
       "      <td>llmjudge-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2005952</td>\n",
       "      <td>0.5039</td>\n",
       "      <td>llmjudge-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2007816</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>llmjudge-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100119</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9052</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100235</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100289</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100399</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100825</td>\n",
       "      <td>0.6347</td>\n",
       "      <td>Olz-gpt4o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2625 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_id      qid   score      judged_by\n",
       "2           naverloo_fs_RR_duo  2002168  0.6996  llmjudge-test\n",
       "12          naverloo_fs_RR_duo  2004282  0.6562  llmjudge-test\n",
       "22          naverloo_fs_RR_duo  2004980  0.3832  llmjudge-test\n",
       "32          naverloo_fs_RR_duo  2005952  0.5039  llmjudge-test\n",
       "42          naverloo_fs_RR_duo  2007816  0.9197  llmjudge-test\n",
       "...                        ...      ...     ...            ...\n",
       "9042  naverloo_bm25_splades_RR  3100119  0.9558      Olz-gpt4o\n",
       "9052  naverloo_bm25_splades_RR  3100235  0.6795      Olz-gpt4o\n",
       "9062  naverloo_bm25_splades_RR  3100289  0.6726      Olz-gpt4o\n",
       "9072  naverloo_bm25_splades_RR  3100399  0.4828      Olz-gpt4o\n",
       "9082  naverloo_bm25_splades_RR  3100825  0.6347      Olz-gpt4o\n",
       "\n",
       "[2625 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_list = []\n",
    "\n",
    "for infile in files:\n",
    "    judger = infile.split('/')[3].split('.')[2]\n",
    "    result_df = pd.read_csv(infile, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    results_df_list.append(result_df)\n",
    " \n",
    "results_dfs = pd.concat(results_df_list)\n",
    "results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_judged = set(results_dfs['qid'])\n",
    "# real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "# t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "# gpt4_queries_judged = [x for x in queries_judged if x > 3100000]\n",
    "\n",
    "# print(len(real_queries_judged))\n",
    "# print(len(t5_queries_judged))\n",
    "# print(len(gpt4_queries_judged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/doc_to_info.txt\", sep='\\t')\n",
    "model_to_info = pd.read_csv(\"infos/model_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(results_dfs, qid_to_info, on='qid')\n",
    "# data = pd.merge(data, doc_to_info, on='qid')\n",
    "data = pd.merge(data, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features that do not seem to be very useful, relevant, or concrete.\n",
    "# QL is a binary feature and we have QW, so it should be dropped.\n",
    "data.drop(['QL'], axis=1, inplace=True)\n",
    "data.drop(['isGPT4'], axis=1, inplace=True)\n",
    "data.drop(['QDR'], axis=1, inplace=True)\n",
    "data.drop(['QDS'], axis=1, inplace=True)\n",
    "data.drop(['isLLM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT'] = data['QT'].astype('category') \n",
    "data['QT'] = data['QT'].replace({0: 'Human', 1: 'T5', 2: 'GPT4'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeler: {'TREMA-rubric0', 'RMITIR-llama70B', 'llmjudge-thomas3', 'llmjudge-simple3', 'NISTRetrieval-instruct0', 'llmjudge-test', \n",
    "# 'TREMA-direct', 'Olz-gpt4o', 'TREMA-4prompts', 'Olz-exp'}\n",
    "\n",
    "data['judged_by'] = data['judged_by'].replace({'TREMA-rubric0': 'Llama3Rubric', 'RMITIR-llama70B': 'Llama3RMIT', 'llmjudge-thomas3': 'GPT4Thomas', 'llmjudge-simple3': 'GPT4Simple', \n",
    "                                               'NISTRetrieval-instruct0': 'Llama3Inst', 'llmjudge-test': 'NIST', 'TREMA-direct': 'FlanT5Direct', 'Olz-gpt4o': 'GPT4oSimple',\n",
    "                                               'TREMA-4prompts': 'Llama3Prompts', 'Olz-exp': 'GPT4oExp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model = \"score ~ QW + APL + MN + C(judged_by, Treatment(reference='NIST')) +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) + C(ST, Treatment(reference='Other')) * C(judged_by, Treatment(reference='NIST')) + QT * C(judged_by, Treatment(reference='NIST'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>score</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>  <td>2625</td>         <td>Method:</td>         <td>REML</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>25</td>          <td>Scale:</td>         <td>0.0360</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>    <td>105</td>     <td>Log-Likelihood:</td>   <td>512.9598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>    <td>105</td>       <td>Converged:</td>         <td>Yes</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>   <td>105.0</td>           <td></td>               <td></td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                                     <td></td>                                                      <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                                                                <td>0.423</td>   <td>0.104</td>   <td>4.062</td> <td>0.000</td>  <td>0.219</td>  <td>0.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>                                                <td>0.232</td>   <td>0.017</td>  <td>13.955</td> <td>0.000</td>  <td>0.199</td>  <td>0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>                                                 <td>0.123</td>   <td>0.017</td>   <td>7.373</td> <td>0.000</td>  <td>0.090</td>  <td>0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]</th>                                                                                                <td>-0.072</td>   <td>0.080</td>  <td>-0.906</td> <td>0.365</td> <td>-0.229</td>  <td>0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]</th>                                                                                               <td>0.061</td>   <td>0.105</td>   <td>0.587</td> <td>0.557</td> <td>-0.143</td>  <td>0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                              <td>-0.062</td>   <td>0.021</td>  <td>-2.968</td> <td>0.003</td> <td>-0.102</td> <td>-0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]</th>                                                                <td>0.013</td>   <td>0.021</td>   <td>0.641</td> <td>0.522</td> <td>-0.027</td>  <td>0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                            <td>0.164</td>   <td>0.019</td>   <td>8.657</td> <td>0.000</td>  <td>0.127</td>  <td>0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                     <td>-0.066</td>   <td>0.028</td>  <td>-2.368</td> <td>0.018</td> <td>-0.121</td> <td>-0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                    <td>0.031</td>   <td>0.028</td>   <td>1.094</td> <td>0.274</td> <td>-0.024</td>  <td>0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                       <td>0.032</td>   <td>0.025</td>   <td>1.244</td> <td>0.214</td> <td>-0.018</td>  <td>0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                    <td>-0.045</td>   <td>0.025</td>  <td>-1.785</td> <td>0.074</td> <td>-0.095</td>  <td>0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                  <td>-0.003</td>   <td>0.025</td>  <td>-0.105</td> <td>0.917</td> <td>-0.052</td>  <td>0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                <td>-0.008</td>   <td>0.025</td>  <td>-0.300</td> <td>0.764</td> <td>-0.057</td>  <td>0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>    <td>-0.043</td>   <td>0.026</td>  <td>-1.639</td> <td>0.101</td> <td>-0.095</td>  <td>0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>      <td>0.017</td>   <td>0.024</td>   <td>0.687</td> <td>0.492</td> <td>-0.031</td>  <td>0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>  <td>0.021</td>   <td>0.024</td>   <td>0.879</td> <td>0.379</td> <td>-0.026</td>  <td>0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>      <td>0.003</td>   <td>0.026</td>   <td>0.115</td> <td>0.908</td> <td>-0.049</td>  <td>0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>       <td>0.019</td>   <td>0.024</td>   <td>0.799</td> <td>0.424</td> <td>-0.028</td>  <td>0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>   <td>0.055</td>   <td>0.024</td>   <td>2.287</td> <td>0.022</td>  <td>0.008</td>  <td>0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>                                      <td>-0.052</td>   <td>0.023</td>  <td>-2.221</td> <td>0.026</td> <td>-0.098</td> <td>-0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>                                    <td>-0.124</td>   <td>0.023</td>  <td>-5.277</td> <td>0.000</td> <td>-0.169</td> <td>-0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>                                       <td>-0.120</td>   <td>0.023</td>  <td>-5.106</td> <td>0.000</td> <td>-0.165</td> <td>-0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>                                     <td>-0.002</td>   <td>0.023</td>  <td>-0.105</td> <td>0.916</td> <td>-0.048</td>  <td>0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QW</th>                                                                                                      <td>-0.010</td>   <td>0.016</td>  <td>-0.618</td> <td>0.536</td> <td>-0.041</td>  <td>0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>APL</th>                                                                                                      <td>0.000</td>   <td>0.000</td>   <td>0.244</td> <td>0.807</td> <td>-0.000</td>  <td>0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MN</th>                                                                                                       <td>0.024</td>   <td>0.002</td>  <td>11.083</td> <td>0.000</td>  <td>0.020</td>  <td>0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group Var</th>                                                                                                <td>0.021</td>   <td>0.036</td>     <td></td>      <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table><br/>\n"
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Mixed Linear Model Regression Results}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:            & MixedLM & Dependent Variable: & score     \\\\\n",
       "No. Observations: & 2625    & Method:             & REML      \\\\\n",
       "No. Groups:       & 25      & Scale:              & 0.0360    \\\\\n",
       "Min. group size:  & 105     & Log-Likelihood:     & 512.9598  \\\\\n",
       "Max. group size:  & 105     & Converged:          & Yes       \\\\\n",
       "Mean group size:  & 105.0   &                     &           \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                                                                                                         &  Coef. & Std.Err. &      z & P$> |$z$|$ & [0.025 & 0.975]  \\\\\n",
       "\\hline\n",
       "Intercept                                                                                                &  0.423 &    0.104 &  4.062 &       0.000 &  0.219 &  0.627  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                               &  0.232 &    0.017 & 13.955 &       0.000 &  0.199 &  0.265  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                &  0.123 &    0.017 &  7.373 &       0.000 &  0.090 &  0.155  \\\\\n",
       "QT[T.T5]                                                                                                 & -0.072 &    0.080 & -0.906 &       0.365 & -0.229 &  0.084  \\\\\n",
       "QT[T.GPT4]                                                                                               &  0.061 &    0.105 &  0.587 &       0.557 & -0.143 &  0.266  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                               & -0.062 &    0.021 & -2.968 &       0.003 & -0.102 & -0.021  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                &  0.013 &    0.021 &  0.641 &       0.522 & -0.027 &  0.054  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                            &  0.164 &    0.019 &  8.657 &       0.000 &  0.127 &  0.201  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                      & -0.066 &    0.028 & -2.368 &       0.018 & -0.121 & -0.011  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                    &  0.031 &    0.028 &  1.094 &       0.274 & -0.024 &  0.085  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                       &  0.032 &    0.025 &  1.244 &       0.214 & -0.018 &  0.081  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                     & -0.045 &    0.025 & -1.785 &       0.074 & -0.095 &  0.004  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                   & -0.003 &    0.025 & -0.105 &       0.917 & -0.052 &  0.047  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                 & -0.008 &    0.025 & -0.300 &       0.764 & -0.057 &  0.042  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]    & -0.043 &    0.026 & -1.639 &       0.101 & -0.095 &  0.008  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]     &  0.017 &    0.024 &  0.687 &       0.492 & -0.031 &  0.064  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct] &  0.021 &    0.024 &  0.879 &       0.379 & -0.026 &  0.068  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]     &  0.003 &    0.026 &  0.115 &       0.908 & -0.049 &  0.055  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]      &  0.019 &    0.024 &  0.799 &       0.424 & -0.028 &  0.067  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]  &  0.055 &    0.024 &  2.287 &       0.022 &  0.008 &  0.102  \\\\\n",
       "QT[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                      & -0.052 &    0.023 & -2.221 &       0.026 & -0.098 & -0.006  \\\\\n",
       "QT[T.GPT4]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                    & -0.124 &    0.023 & -5.277 &       0.000 & -0.169 & -0.078  \\\\\n",
       "QT[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                       & -0.120 &    0.023 & -5.106 &       0.000 & -0.165 & -0.074  \\\\\n",
       "QT[T.GPT4]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                     & -0.002 &    0.023 & -0.105 &       0.916 & -0.048 &  0.043  \\\\\n",
       "QW                                                                                                       & -0.010 &    0.016 & -0.618 &       0.536 & -0.041 &  0.021  \\\\\n",
       "APL                                                                                                      &  0.000 &    0.000 &  0.244 &       0.807 & -0.000 &  0.000  \\\\\n",
       "MN                                                                                                       &  0.024 &    0.002 & 11.083 &       0.000 &  0.020 &  0.029  \\\\\n",
       "Group Var                                                                                                &  0.021 &    0.036 &        &             &        &         \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                                      Mixed Linear Model Regression Results\n",
       "==================================================================================================================================================\n",
       "Model:                                            MixedLM                               Dependent Variable:                               score   \n",
       "No. Observations:                                 2625                                  Method:                                           REML    \n",
       "No. Groups:                                       25                                    Scale:                                            0.0360  \n",
       "Min. group size:                                  105                                   Log-Likelihood:                                   512.9598\n",
       "Max. group size:                                  105                                   Converged:                                        Yes     \n",
       "Mean group size:                                  105.0                                                                                           \n",
       "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "                                                                                                        Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
       "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                                                                0.423    0.104  4.062 0.000  0.219  0.627\n",
       "C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                                0.232    0.017 13.955 0.000  0.199  0.265\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                 0.123    0.017  7.373 0.000  0.090  0.155\n",
       "QT[T.T5]                                                                                                -0.072    0.080 -0.906 0.365 -0.229  0.084\n",
       "QT[T.GPT4]                                                                                               0.061    0.105  0.587 0.557 -0.143  0.266\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                              -0.062    0.021 -2.968 0.003 -0.102 -0.021\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                0.013    0.021  0.641 0.522 -0.027  0.054\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                            0.164    0.019  8.657 0.000  0.127  0.201\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                     -0.066    0.028 -2.368 0.018 -0.121 -0.011\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                    0.031    0.028  1.094 0.274 -0.024  0.085\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                       0.032    0.025  1.244 0.214 -0.018  0.081\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                    -0.045    0.025 -1.785 0.074 -0.095  0.004\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                  -0.003    0.025 -0.105 0.917 -0.052  0.047\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                -0.008    0.025 -0.300 0.764 -0.057  0.042\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]    -0.043    0.026 -1.639 0.101 -0.095  0.008\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]      0.017    0.024  0.687 0.492 -0.031  0.064\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]  0.021    0.024  0.879 0.379 -0.026  0.068\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]      0.003    0.026  0.115 0.908 -0.049  0.055\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]       0.019    0.024  0.799 0.424 -0.028  0.067\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]   0.055    0.024  2.287 0.022  0.008  0.102\n",
       "QT[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                      -0.052    0.023 -2.221 0.026 -0.098 -0.006\n",
       "QT[T.GPT4]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                    -0.124    0.023 -5.277 0.000 -0.169 -0.078\n",
       "QT[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                       -0.120    0.023 -5.106 0.000 -0.165 -0.074\n",
       "QT[T.GPT4]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                     -0.002    0.023 -0.105 0.916 -0.048  0.043\n",
       "QW                                                                                                      -0.010    0.016 -0.618 0.536 -0.041  0.021\n",
       "APL                                                                                                      0.000    0.000  0.244 0.807 -0.000  0.000\n",
       "MN                                                                                                       0.024    0.002 11.083 0.000  0.020  0.029\n",
       "Group Var                                                                                                0.021    0.036                           \n",
       "==================================================================================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model, data, groups=data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['QD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_data = data2.pivot(index=['run_id', 'qid', 'QW', 'APL', 'QT', 'ST', 'pipeline'], columns=['judged_by'], values='score')\n",
    "score_diff_data.columns.name = None\n",
    "score_diff_data = score_diff_data.reset_index()\n",
    "score_diff_data['score_diff'] = score_diff_data['gpt4'] - score_diff_data['nist']\n",
    "score_diff_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_mixed_model = \"score_diff ~  QW + APL + pipeline  +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(score_diff_mixed_model, score_diff_data, groups=score_diff_data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_condition_Qreal = \"score ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\"\n",
    "mixed_model_condition_Qsynthetic = \"score ~ QL + QDS + QW + C(LLM, Treatment(reference='Other')) + pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_real_judgments, groups=real_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_real_judgments, groups=synthetic_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_synthetic_judgments, groups=real_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_synthetic_judgments, groups=synthetic_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "real_queries_diff = pd.merge(real_queries_real_judgments, real_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "real_queries_diff['score_ab'] = real_queries_diff['score_a'] - real_queries_diff['score_b']\n",
    "real_queries_diff['score_ba'] = real_queries_diff['score_b'] - real_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "synthetic_queries_diff = pd.merge(synthetic_queries_real_judgments, synthetic_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "synthetic_queries_diff['score_ab'] = synthetic_queries_diff['score_a'] - synthetic_queries_diff['score_b']\n",
    "synthetic_queries_diff['score_ba'] = synthetic_queries_diff['score_b'] - synthetic_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments (refer to: \"Extra Exp. 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_file):\n",
    "    result_df = pd.read_csv(result_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_judge_results = get_result(result_file=\"results/all.pass.nist.ndcgeval\")\n",
    "synthetic_judge_results = get_result(result_file=\"results/all.pass.gpt4.ndcgeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "results_diff = pd.merge(real_judge_results, synthetic_judge_results, on=['run_id', 'qid'], suffixes=('_real', '_synthetic'))\n",
    "# Subtracting the 'Score' values\n",
    "results_diff['score_RS'] = results_diff['score_real'] - results_diff['score_synthetic']\n",
    "results_diff['score_SR'] = results_diff['score_synthetic'] - results_diff['score_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff = pd.merge(results_diff, qid_to_info, on='qid')\n",
    "results_diff = pd.merge(results_diff, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected: score_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff['qd_RS'] = results_diff['QDR'] - results_diff['QDS']\n",
    "results_diff['qd_SR'] = results_diff['QDS'] - results_diff['QDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_1 = \"score_SR ~ Synthetic + QL + qd_SR + QW + DL + pipeline + C(LLM, Treatment(reference='Other')) + Synthetic * C(LLM, Treatment(reference='Other'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_1, results_diff, groups=results_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
