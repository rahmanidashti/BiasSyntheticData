{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"ndcg_cut_10\" # map # ndcg_cut_10\n",
    "result_format = \"ndcgeval\" # treceval # ndcgeval\n",
    "results_from = \"LLMJudge2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'./results/{results_from}/*.{result_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./results/LLMJudge2024/all.pass.TREMA-rubric0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.RMITIR-llama70B.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-thomas3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-simple3.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.NISTRetrieval-instruct0.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.llmjudge-test.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-direct.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-gpt4o.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.TREMA-4prompts.ndcgeval',\n",
       " './results/LLMJudge2024/all.pass.Olz-exp.ndcgeval']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for removed_labler in ['RMITIR-llama70B', 'llmjudge-simple3', 'Olz-exp', 'TREMA-4prompts', 'TREMA-direct']:\n",
    "    removed_labler = f'./results/LLMJudge2024/all.pass.{removed_labler}.{result_format}'\n",
    "    files.remove(removed_labler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>score</th>\n",
       "      <th>judged_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2002168</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004282</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2004980</td>\n",
       "      <td>0.4221</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2005952</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>naverloo_fs_RR_duo</td>\n",
       "      <td>2007816</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>TREMA-rubric0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100119</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>Olz-exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9052</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100235</td>\n",
       "      <td>0.6217</td>\n",
       "      <td>Olz-exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100289</td>\n",
       "      <td>0.7046</td>\n",
       "      <td>Olz-exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100399</td>\n",
       "      <td>0.5089</td>\n",
       "      <td>Olz-exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>naverloo_bm25_splades_RR</td>\n",
       "      <td>3100825</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>Olz-exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8750 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run_id      qid   score      judged_by\n",
       "2           naverloo_fs_RR_duo  2002168  1.0000  TREMA-rubric0\n",
       "12          naverloo_fs_RR_duo  2004282  0.3396  TREMA-rubric0\n",
       "22          naverloo_fs_RR_duo  2004980  0.4221  TREMA-rubric0\n",
       "32          naverloo_fs_RR_duo  2005952  0.6944  TREMA-rubric0\n",
       "42          naverloo_fs_RR_duo  2007816  0.6512  TREMA-rubric0\n",
       "...                        ...      ...     ...            ...\n",
       "9042  naverloo_bm25_splades_RR  3100119  0.8665        Olz-exp\n",
       "9052  naverloo_bm25_splades_RR  3100235  0.6217        Olz-exp\n",
       "9062  naverloo_bm25_splades_RR  3100289  0.7046        Olz-exp\n",
       "9072  naverloo_bm25_splades_RR  3100399  0.5089        Olz-exp\n",
       "9082  naverloo_bm25_splades_RR  3100825  0.5612        Olz-exp\n",
       "\n",
       "[8750 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_list = []\n",
    "\n",
    "for infile in glob.glob(f'./results/{results_from}/*.{result_format}'):\n",
    "    judger = infile.split('/')[3].split('.')[2]\n",
    "    result_df = pd.read_csv(infile, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    results_df_list.append(result_df)\n",
    " \n",
    "results_dfs = pd.concat(results_df_list)\n",
    "results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_judged = set(results_dfs['qid'])\n",
    "# real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "# t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "# gpt4_queries_judged = [x for x in queries_judged if x > 3100000]\n",
    "\n",
    "# print(len(real_queries_judged))\n",
    "# print(len(t5_queries_judged))\n",
    "# print(len(gpt4_queries_judged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/doc_to_info.txt\", sep='\\t')\n",
    "model_to_info = pd.read_csv(\"infos/model_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(results_dfs, qid_to_info, on='qid')\n",
    "# data = pd.merge(data, doc_to_info, on='qid')\n",
    "data = pd.merge(data, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features that do not seem to be very useful, relevant, or concrete.\n",
    "# QL is a binary feature and we have QW, so it should be dropped.\n",
    "data.drop(['QL'], axis=1, inplace=True)\n",
    "data.drop(['isGPT4'], axis=1, inplace=True)\n",
    "data.drop(['QDR'], axis=1, inplace=True)\n",
    "data.drop(['QDS'], axis=1, inplace=True)\n",
    "data.drop(['isLLM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT'] = data['QT'].astype('category') \n",
    "data['QT'] = data['QT'].replace({0: 'Human', 1: 'T5', 2: 'GPT4'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeler: {'TREMA-rubric0', 'RMITIR-llama70B', 'llmjudge-thomas3', 'llmjudge-simple3', 'NISTRetrieval-instruct0', 'llmjudge-test', \n",
    "# 'TREMA-direct', 'Olz-gpt4o', 'TREMA-4prompts', 'Olz-exp'}\n",
    "\n",
    "data['judged_by'] = data['judged_by'].replace({'TREMA-rubric0': 'Llama3Rubric', 'RMITIR-llama70B': 'Llama3RMIT', 'llmjudge-thomas3': 'GPT4Thomas', 'llmjudge-simple3': 'GPT4Simple', \n",
    "                                               'NISTRetrieval-instruct0': 'Llama3Inst', 'llmjudge-test': 'NIST', 'TREMA-direct': 'FlanT5Direct', 'Olz-gpt4o': 'GPT4oSimple',\n",
    "                                               'TREMA-4prompts': 'Llama3Prompts', 'Olz-exp': 'GPT4oExp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model = \"score ~ QW + APL + MN + C(judged_by, Treatment(reference='NIST')) +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) + C(ST, Treatment(reference='Other')) * C(judged_by, Treatment(reference='NIST'))  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>score</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>No. Observations:</td>  <td>8750</td>         <td>Method:</td>         <td>REML</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>No. Groups:</td>      <td>25</td>          <td>Scale:</td>         <td>0.0363</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Min. group size:</td>    <td>350</td>     <td>Log-Likelihood:</td>   <td>1876.4213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Max. group size:</td>    <td>350</td>       <td>Converged:</td>         <td>Yes</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Mean group size:</td>   <td>350.0</td>           <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                                      <td></td>                                                      <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                                                                 <td>0.454</td>   <td>0.101</td>   <td>4.505</td> <td>0.000</td>  <td>0.257</td>  <td>0.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>                                                 <td>0.197</td>   <td>0.015</td>  <td>13.170</td> <td>0.000</td>  <td>0.168</td>  <td>0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]</th>                                                   <td>0.180</td>   <td>0.015</td>  <td>12.033</td> <td>0.000</td>  <td>0.151</td>  <td>0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>                                                   <td>0.181</td>   <td>0.015</td>  <td>12.125</td> <td>0.000</td>  <td>0.152</td>  <td>0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]</th>                                                     <td>0.087</td>   <td>0.015</td>   <td>5.817</td> <td>0.000</td>  <td>0.058</td>  <td>0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>                                                  <td>0.098</td>   <td>0.015</td>   <td>6.566</td> <td>0.000</td>  <td>0.069</td>  <td>0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>                                                   <td>0.210</td>   <td>0.015</td>  <td>14.046</td> <td>0.000</td>  <td>0.181</td>  <td>0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]</th>                                                <td>0.213</td>   <td>0.015</td>  <td>14.263</td> <td>0.000</td>  <td>0.184</td>  <td>0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]</th>                                                   <td>0.191</td>   <td>0.015</td>  <td>12.782</td> <td>0.000</td>  <td>0.162</td>  <td>0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>                                                <td>-0.027</td>   <td>0.015</td>  <td>-1.830</td> <td>0.067</td> <td>-0.057</td>  <td>0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]</th>                                                                                                 <td>-0.108</td>   <td>0.076</td>  <td>-1.423</td> <td>0.155</td> <td>-0.256</td>  <td>0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]</th>                                                                                                <td>0.063</td>   <td>0.100</td>   <td>0.631</td> <td>0.528</td> <td>-0.133</td>  <td>0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                               <td>-0.053</td>   <td>0.019</td>  <td>-2.711</td> <td>0.007</td> <td>-0.091</td> <td>-0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]</th>                                                                 <td>0.024</td>   <td>0.018</td>   <td>1.326</td> <td>0.185</td> <td>-0.012</td>  <td>0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                             <td>0.174</td>   <td>0.018</td>   <td>9.821</td> <td>0.000</td>  <td>0.139</td>  <td>0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                      <td>-0.077</td>   <td>0.015</td>  <td>-5.006</td> <td>0.000</td> <td>-0.107</td> <td>-0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]</th>                                                    <td>-0.007</td>   <td>0.015</td>  <td>-0.470</td> <td>0.639</td> <td>-0.037</td>  <td>0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                        <td>0.011</td>   <td>0.014</td>   <td>0.819</td> <td>0.413</td> <td>-0.016</td>  <td>0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]</th>                                                     <td>-0.051</td>   <td>0.014</td>  <td>-3.647</td> <td>0.000</td> <td>-0.078</td> <td>-0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                   <td>-0.034</td>   <td>0.014</td>  <td>-2.415</td> <td>0.016</td> <td>-0.061</td> <td>-0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]</th>                                                 <td>-0.019</td>   <td>0.014</td>  <td>-1.382</td> <td>0.167</td> <td>-0.047</td>  <td>0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>     <td>-0.043</td>   <td>0.027</td>  <td>-1.632</td> <td>0.103</td> <td>-0.096</td>  <td>0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>       <td>0.017</td>   <td>0.024</td>   <td>0.684</td> <td>0.494</td> <td>-0.031</td>  <td>0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]</th>   <td>0.021</td>   <td>0.024</td>   <td>0.875</td> <td>0.381</td> <td>-0.026</td>  <td>0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]</th>        <td>0.003</td>   <td>0.027</td>   <td>0.128</td> <td>0.898</td> <td>-0.049</td>  <td>0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]</th>         <td>0.001</td>   <td>0.024</td>   <td>0.047</td> <td>0.963</td> <td>-0.046</td>  <td>0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]</th>     <td>0.024</td>   <td>0.024</td>   <td>0.996</td> <td>0.319</td> <td>-0.023</td>  <td>0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>       <td>-0.011</td>   <td>0.027</td>  <td>-0.430</td> <td>0.667</td> <td>-0.064</td>  <td>0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>         <td>0.012</td>   <td>0.024</td>   <td>0.498</td> <td>0.619</td> <td>-0.035</td>  <td>0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]</th>     <td>0.032</td>   <td>0.024</td>   <td>1.340</td> <td>0.180</td> <td>-0.015</td>  <td>0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]</th>         <td>-0.012</td>   <td>0.027</td>  <td>-0.457</td> <td>0.648</td> <td>-0.064</td>  <td>0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]</th>           <td>0.016</td>   <td>0.024</td>   <td>0.649</td> <td>0.516</td> <td>-0.032</td>  <td>0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]</th>       <td>0.029</td>   <td>0.024</td>   <td>1.203</td> <td>0.229</td> <td>-0.018</td>  <td>0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>       <td>0.003</td>   <td>0.027</td>   <td>0.115</td> <td>0.909</td> <td>-0.049</td>  <td>0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>        <td>0.019</td>   <td>0.024</td>   <td>0.796</td> <td>0.426</td> <td>-0.028</td>  <td>0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]</th>    <td>0.055</td>   <td>0.024</td>   <td>2.277</td> <td>0.023</td>  <td>0.008</td>  <td>0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>        <td>0.002</td>   <td>0.027</td>   <td>0.058</td> <td>0.954</td> <td>-0.051</td>  <td>0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>        <td>-0.005</td>   <td>0.024</td>  <td>-0.206</td> <td>0.837</td> <td>-0.052</td>  <td>0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]</th>    <td>-0.014</td>   <td>0.024</td>  <td>-0.585</td> <td>0.558</td> <td>-0.062</td>  <td>0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]</th>     <td>0.018</td>   <td>0.027</td>   <td>0.695</td> <td>0.487</td> <td>-0.034</td>  <td>0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]</th>     <td>-0.009</td>   <td>0.024</td>  <td>-0.364</td> <td>0.716</td> <td>-0.056</td>  <td>0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]</th> <td>-0.009</td>   <td>0.024</td>  <td>-0.368</td> <td>0.713</td> <td>-0.056</td>  <td>0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]</th>        <td>0.002</td>   <td>0.027</td>   <td>0.061</td> <td>0.951</td> <td>-0.050</td>  <td>0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]</th>         <td>0.006</td>   <td>0.024</td>   <td>0.260</td> <td>0.795</td> <td>-0.041</td>  <td>0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]</th>     <td>0.012</td>   <td>0.024</td>   <td>0.491</td> <td>0.624</td> <td>-0.036</td>  <td>0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>      <td>0.001</td>   <td>0.027</td>   <td>0.041</td> <td>0.967</td> <td>-0.051</td>  <td>0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>      <td>-0.047</td>   <td>0.024</td>  <td>-1.946</td> <td>0.052</td> <td>-0.095</td>  <td>0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]</th>  <td>-0.061</td>   <td>0.024</td>  <td>-2.508</td> <td>0.012</td> <td>-0.108</td> <td>-0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>QW</th>                                                                                                       <td>-0.011</td>   <td>0.015</td>  <td>-0.724</td> <td>0.469</td> <td>-0.041</td>  <td>0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>APL</th>                                                                                                      <td>-0.000</td>   <td>0.000</td>  <td>-0.158</td> <td>0.874</td> <td>-0.000</td>  <td>0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MN</th>                                                                                                        <td>0.023</td>   <td>0.001</td>  <td>18.909</td> <td>0.000</td>  <td>0.020</td>  <td>0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group Var</th>                                                                                                 <td>0.020</td>   <td>0.034</td>     <td></td>      <td></td>       <td></td>       <td></td>   \n",
       "</tr>\n",
       "</table><br/>\n"
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Mixed Linear Model Regression Results}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:            & MixedLM & Dependent Variable: & score      \\\\\n",
       "No. Observations: & 8750    & Method:             & REML       \\\\\n",
       "No. Groups:       & 25      & Scale:              & 0.0363     \\\\\n",
       "Min. group size:  & 350     & Log-Likelihood:     & 1876.4213  \\\\\n",
       "Max. group size:  & 350     & Converged:          & Yes        \\\\\n",
       "Mean group size:  & 350.0   &                     &            \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                                                                                                          &  Coef. & Std.Err. &      z & P$> |$z$|$ & [0.025 & 0.975]  \\\\\n",
       "\\hline\n",
       "Intercept                                                                                                 &  0.454 &    0.101 &  4.505 &       0.000 &  0.257 &  0.652  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                                &  0.197 &    0.015 & 13.170 &       0.000 &  0.168 &  0.226  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Simple]                                                  &  0.180 &    0.015 & 12.033 &       0.000 &  0.151 &  0.209  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]                                                  &  0.181 &    0.015 & 12.125 &       0.000 &  0.152 &  0.210  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oExp]                                                    &  0.087 &    0.015 &  5.817 &       0.000 &  0.058 &  0.116  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                 &  0.098 &    0.015 &  6.566 &       0.000 &  0.069 &  0.127  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]                                                  &  0.210 &    0.015 & 14.046 &       0.000 &  0.181 &  0.239  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Prompts]                                               &  0.213 &    0.015 & 14.263 &       0.000 &  0.184 &  0.242  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3RMIT]                                                  &  0.191 &    0.015 & 12.782 &       0.000 &  0.162 &  0.220  \\\\\n",
       "C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]                                                & -0.027 &    0.015 & -1.830 &       0.067 & -0.057 &  0.002  \\\\\n",
       "QT[T.T5]                                                                                                  & -0.108 &    0.076 & -1.423 &       0.155 & -0.256 &  0.041  \\\\\n",
       "QT[T.GPT4]                                                                                                &  0.063 &    0.100 &  0.631 &       0.528 & -0.133 &  0.259  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                                & -0.053 &    0.019 & -2.711 &       0.007 & -0.091 & -0.015  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                 &  0.024 &    0.018 &  1.326 &       0.185 & -0.012 &  0.060  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                             &  0.174 &    0.018 &  9.821 &       0.000 &  0.139 &  0.209  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                       & -0.077 &    0.015 & -5.006 &       0.000 & -0.107 & -0.047  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                     & -0.007 &    0.015 & -0.470 &       0.639 & -0.037 &  0.023  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                        &  0.011 &    0.014 &  0.819 &       0.413 & -0.016 &  0.039  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                      & -0.051 &    0.014 & -3.647 &       0.000 & -0.078 & -0.024  \\\\\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                    & -0.034 &    0.014 & -2.415 &       0.016 & -0.061 & -0.006  \\\\\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                  & -0.019 &    0.014 & -1.382 &       0.167 & -0.047 &  0.008  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]     & -0.043 &    0.027 & -1.632 &       0.103 & -0.096 &  0.009  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]      &  0.017 &    0.024 &  0.684 &       0.494 & -0.031 &  0.064  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.FlanT5Direct]  &  0.021 &    0.024 &  0.875 &       0.381 & -0.026 &  0.069  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Simple]       &  0.003 &    0.027 &  0.128 &       0.898 & -0.049 &  0.056  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Simple]        &  0.001 &    0.024 &  0.047 &       0.963 & -0.046 &  0.049  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Simple]    &  0.024 &    0.024 &  0.996 &       0.319 & -0.023 &  0.072  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]       & -0.011 &    0.027 & -0.430 &       0.667 & -0.064 &  0.041  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]        &  0.012 &    0.024 &  0.498 &       0.619 & -0.035 &  0.060  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4Thomas]    &  0.032 &    0.024 &  1.340 &       0.180 & -0.015 &  0.080  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oExp]         & -0.012 &    0.027 & -0.457 &       0.648 & -0.064 &  0.040  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oExp]          &  0.016 &    0.024 &  0.649 &       0.516 & -0.032 &  0.063  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oExp]      &  0.029 &    0.024 &  1.203 &       0.229 & -0.018 &  0.077  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]      &  0.003 &    0.027 &  0.115 &       0.909 & -0.049 &  0.055  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]       &  0.019 &    0.024 &  0.796 &       0.426 & -0.028 &  0.067  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.GPT4oSimple]   &  0.055 &    0.024 &  2.277 &       0.023 &  0.008 &  0.103  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]       &  0.002 &    0.027 &  0.058 &       0.954 & -0.051 &  0.054  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]        & -0.005 &    0.024 & -0.206 &       0.837 & -0.052 &  0.042  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Inst]    & -0.014 &    0.024 & -0.585 &       0.558 & -0.062 &  0.033  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Prompts]    &  0.018 &    0.027 &  0.695 &       0.487 & -0.034 &  0.071  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Prompts]     & -0.009 &    0.024 & -0.364 &       0.716 & -0.056 &  0.039  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Prompts] & -0.009 &    0.024 & -0.368 &       0.713 & -0.056 &  0.039  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3RMIT]       &  0.002 &    0.027 &  0.061 &       0.951 & -0.050 &  0.054  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3RMIT]        &  0.006 &    0.024 &  0.260 &       0.795 & -0.041 &  0.054  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3RMIT]    &  0.012 &    0.024 &  0.491 &       0.624 & -0.036 &  0.059  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]     &  0.001 &    0.027 &  0.041 &       0.967 & -0.051 &  0.053  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]      & -0.047 &    0.024 & -1.946 &       0.052 & -0.095 &  0.000  \\\\\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged\\_by, Treatment(reference='NIST'))[T.Llama3Rubric]  & -0.061 &    0.024 & -2.508 &       0.012 & -0.108 & -0.013  \\\\\n",
       "QW                                                                                                        & -0.011 &    0.015 & -0.724 &       0.469 & -0.041 &  0.019  \\\\\n",
       "APL                                                                                                       & -0.000 &    0.000 & -0.158 &       0.874 & -0.000 &  0.000  \\\\\n",
       "MN                                                                                                        &  0.023 &    0.001 & 18.909 &       0.000 &  0.020 &  0.025  \\\\\n",
       "Group Var                                                                                                 &  0.020 &    0.034 &        &             &        &         \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                                       Mixed Linear Model Regression Results\n",
       "===================================================================================================================================================\n",
       "Model:                                            MixedLM                               Dependent Variable:                               score    \n",
       "No. Observations:                                 8750                                  Method:                                           REML     \n",
       "No. Groups:                                       25                                    Scale:                                            0.0363   \n",
       "Min. group size:                                  350                                   Log-Likelihood:                                   1876.4213\n",
       "Max. group size:                                  350                                   Converged:                                        Yes      \n",
       "Mean group size:                                  350.0                                                                                            \n",
       "---------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "                                                                                                         Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
       "---------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                                                                 0.454    0.101  4.505 0.000  0.257  0.652\n",
       "C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]                                                 0.197    0.015 13.170 0.000  0.168  0.226\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]                                                   0.180    0.015 12.033 0.000  0.151  0.209\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]                                                   0.181    0.015 12.125 0.000  0.152  0.210\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]                                                     0.087    0.015  5.817 0.000  0.058  0.116\n",
       "C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]                                                  0.098    0.015  6.566 0.000  0.069  0.127\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]                                                   0.210    0.015 14.046 0.000  0.181  0.239\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]                                                0.213    0.015 14.263 0.000  0.184  0.242\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]                                                   0.191    0.015 12.782 0.000  0.162  0.220\n",
       "C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]                                                -0.027    0.015 -1.830 0.067 -0.057  0.002\n",
       "QT[T.T5]                                                                                                 -0.108    0.076 -1.423 0.155 -0.256  0.041\n",
       "QT[T.GPT4]                                                                                                0.063    0.100  0.631 0.528 -0.133  0.259\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]                                                               -0.053    0.019 -2.711 0.007 -0.091 -0.015\n",
       "C(ST, Treatment(reference='Other'))[T.T5]                                                                 0.024    0.018  1.326 0.185 -0.012  0.060\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                             0.174    0.018  9.821 0.000  0.139  0.209\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.GPT]                                                      -0.077    0.015 -5.006 0.000 -0.107 -0.047\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.GPT]                                                    -0.007    0.015 -0.470 0.639 -0.037  0.023\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5]                                                        0.011    0.014  0.819 0.413 -0.016  0.039\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5]                                                     -0.051    0.014 -3.647 0.000 -0.078 -0.024\n",
       "QT[T.T5]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                   -0.034    0.014 -2.415 0.016 -0.061 -0.006\n",
       "QT[T.GPT4]:C(ST, Treatment(reference='Other'))[T.T5+GPT]                                                 -0.019    0.014 -1.382 0.167 -0.047  0.008\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]     -0.043    0.027 -1.632 0.103 -0.096  0.009\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]       0.017    0.024  0.684 0.494 -0.031  0.064\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.FlanT5Direct]   0.021    0.024  0.875 0.381 -0.026  0.069\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]        0.003    0.027  0.128 0.898 -0.049  0.056\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]         0.001    0.024  0.047 0.963 -0.046  0.049\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Simple]     0.024    0.024  0.996 0.319 -0.023  0.072\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]       -0.011    0.027 -0.430 0.667 -0.064  0.041\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]         0.012    0.024  0.498 0.619 -0.035  0.060\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4Thomas]     0.032    0.024  1.340 0.180 -0.015  0.080\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]         -0.012    0.027 -0.457 0.648 -0.064  0.040\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]           0.016    0.024  0.649 0.516 -0.032  0.063\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oExp]       0.029    0.024  1.203 0.229 -0.018  0.077\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]       0.003    0.027  0.115 0.909 -0.049  0.055\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]        0.019    0.024  0.796 0.426 -0.028  0.067\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.GPT4oSimple]    0.055    0.024  2.277 0.023  0.008  0.103\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]        0.002    0.027  0.058 0.954 -0.051  0.054\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]        -0.005    0.024 -0.206 0.837 -0.052  0.042\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Inst]    -0.014    0.024 -0.585 0.558 -0.062  0.033\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]     0.018    0.027  0.695 0.487 -0.034  0.071\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts]     -0.009    0.024 -0.364 0.716 -0.056  0.039\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Prompts] -0.009    0.024 -0.368 0.713 -0.056  0.039\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]        0.002    0.027  0.061 0.951 -0.050  0.054\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]         0.006    0.024  0.260 0.795 -0.041  0.054\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3RMIT]     0.012    0.024  0.491 0.624 -0.036  0.059\n",
       "C(ST, Treatment(reference='Other'))[T.GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]      0.001    0.027  0.041 0.967 -0.051  0.053\n",
       "C(ST, Treatment(reference='Other'))[T.T5]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]      -0.047    0.024 -1.946 0.052 -0.095  0.000\n",
       "C(ST, Treatment(reference='Other'))[T.T5+GPT]:C(judged_by, Treatment(reference='NIST'))[T.Llama3Rubric]  -0.061    0.024 -2.508 0.012 -0.108 -0.013\n",
       "QW                                                                                                       -0.011    0.015 -0.724 0.469 -0.041  0.019\n",
       "APL                                                                                                      -0.000    0.000 -0.158 0.874 -0.000  0.000\n",
       "MN                                                                                                        0.023    0.001 18.909 0.000  0.020  0.025\n",
       "Group Var                                                                                                 0.020    0.034                           \n",
       "===================================================================================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model, data, groups=data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['QD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_data = data2.pivot(index=['run_id', 'qid', 'QW', 'APL', 'QT', 'ST', 'pipeline'], columns=['judged_by'], values='score')\n",
    "score_diff_data.columns.name = None\n",
    "score_diff_data = score_diff_data.reset_index()\n",
    "score_diff_data['score_diff'] = score_diff_data['gpt4'] - score_diff_data['nist']\n",
    "score_diff_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff_mixed_model = \"score_diff ~  QW + APL + pipeline  +  QT + C(ST, Treatment(reference='Other')) + QT * C(ST, Treatment(reference='Other')) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(score_diff_mixed_model, score_diff_data, groups=score_diff_data[\"qid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_condition_Qreal = \"score ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\"\n",
    "mixed_model_condition_Qsynthetic = \"score ~ QL + QDS + QW + C(LLM, Treatment(reference='Other')) + pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_real_judgments, groups=real_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_real_judgments, groups=synthetic_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_synthetic_judgments, groups=real_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_synthetic_judgments, groups=synthetic_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "real_queries_diff = pd.merge(real_queries_real_judgments, real_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "real_queries_diff['score_ab'] = real_queries_diff['score_a'] - real_queries_diff['score_b']\n",
    "real_queries_diff['score_ba'] = real_queries_diff['score_b'] - real_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "synthetic_queries_diff = pd.merge(synthetic_queries_real_judgments, synthetic_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "synthetic_queries_diff['score_ab'] = synthetic_queries_diff['score_a'] - synthetic_queries_diff['score_b']\n",
    "synthetic_queries_diff['score_ba'] = synthetic_queries_diff['score_b'] - synthetic_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments (refer to: \"Extra Exp. 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_file):\n",
    "    result_df = pd.read_csv(result_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_judge_results = get_result(result_file=\"results/all.pass.nist.ndcgeval\")\n",
    "synthetic_judge_results = get_result(result_file=\"results/all.pass.gpt4.ndcgeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "results_diff = pd.merge(real_judge_results, synthetic_judge_results, on=['run_id', 'qid'], suffixes=('_real', '_synthetic'))\n",
    "# Subtracting the 'Score' values\n",
    "results_diff['score_RS'] = results_diff['score_real'] - results_diff['score_synthetic']\n",
    "results_diff['score_SR'] = results_diff['score_synthetic'] - results_diff['score_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff = pd.merge(results_diff, qid_to_info, on='qid')\n",
    "results_diff = pd.merge(results_diff, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected: score_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff['qd_RS'] = results_diff['QDR'] - results_diff['QDS']\n",
    "results_diff['qd_SR'] = results_diff['QDS'] - results_diff['QDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_1 = \"score_SR ~ Synthetic + QL + qd_SR + QW + DL + pipeline + C(LLM, Treatment(reference='Other')) + Synthetic * C(LLM, Treatment(reference='Other'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_1, results_diff, groups=results_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
