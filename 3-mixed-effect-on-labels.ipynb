{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Judgment - Human Judgment as a Target\n",
    "\n",
    "This experiments applied the signed differecnes of LLM labels and human labels as a target for analysis the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for Model Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check model assumptions \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_residuals(result):\n",
    "    \n",
    "    # Extract residuals and fitted values\n",
    "    residuals = result.resid\n",
    "    fitted_values = result.fittedvalues\n",
    "\n",
    "    # Q-Q plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot')\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram of residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='k')\n",
    "    plt.title('Histogram of Residuals')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals vs. Fitted values\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(fitted_values, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Fitted Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(result):\n",
    "    \n",
    "    # Step 1: Extract the coefficients and p-values\n",
    "    coefficients = result.params\n",
    "    p_values = result.pvalues\n",
    "\n",
    "    print(\"Coefficients from the model:\")\n",
    "    print(\"P-values from the model:\")\n",
    "\n",
    "    # Step 2: Plot the coefficients with p-values\n",
    "    # Convert coefficients and p-values to a DataFrame for easy plotting\n",
    "    coeff_df = pd.DataFrame({\n",
    "        'coefficients': coefficients,\n",
    "        'p_values': p_values,\n",
    "        'features': coefficients.index\n",
    "    })\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.barh(coeff_df['features'], coeff_df['coefficients'], color='skyblue')\n",
    "\n",
    "    # Add the p-values as text annotations next to each bar\n",
    "    for bar, p_value in zip(bars, coeff_df['p_values']):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'p = {p_value:.3f}', \n",
    "                va='center', ha='left', fontsize=10)\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Extracted Coefficients with P-Values from the Model')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Features')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_from = \"LLMJudge2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trec_qrels():\n",
    "    # Load each text file into a DataFrame\n",
    "    qrel_human_df = pd.read_csv('./qrels/2023.qrels.pass.withDupes.txt', delimiter=' ', header=None, names=['qid', 'Q0', 'docid', 'label'])  # adjust delimiter if necessary\n",
    "    qrel_llm_df = pd.read_csv('./qrels/2023.qrels.pass.gpt4.txt', delimiter=' ', header=None, names=['qid', 'Q0', 'docid', 'label'])\n",
    "\n",
    "    # Merge the two DataFrames based on a common column, e.g., 'id'\n",
    "    qrels_df = pd.merge(qrel_human_df, qrel_llm_df, on=['qid', 'docid'])\n",
    "\n",
    "    qrels_df.drop(['Q0_x'], axis=1, inplace=True)\n",
    "    qrels_df.drop(['Q0_y'], axis=1, inplace=True)\n",
    "\n",
    "    qrels_df.rename(columns={'label_x': 'label_human'}, inplace=True)\n",
    "    qrels_df.rename(columns={'label_y': 'label_llm'}, inplace=True)\n",
    "\n",
    "    qrels_df['llm_human_diff'] = qrels_df['label_llm'] - qrels_df['label_human']\n",
    "\n",
    "    # Display or save the merged DataFrame\n",
    "    # print(qrels_df['llm-human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = glob.glob(f'./qrels/{results_from}/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for removed_labler in ['RMITIR-llama70B', 'llmjudge-simple3', 'Olz-exp', 'TREMA-4prompts', 'TREMA-direct']:\n",
    "    removed_labler = f'./qrels/LLMJudge2024/{removed_labler}.txt'\n",
    "    qrels.remove(removed_labler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>score</th>\n",
       "      <th>judged_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662987062</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662989674</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_01_366786438</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_01_366787276</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_67_786441513</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_67_797018148</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_250089795</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_682638740</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid                         docid  score     judged_by\n",
       "0     2002168  msmarco_passage_00_662986293      3  LLMJudge2024\n",
       "1     2002168  msmarco_passage_00_662987062      2  LLMJudge2024\n",
       "2     2002168  msmarco_passage_00_662989674      3  LLMJudge2024\n",
       "3     2002168  msmarco_passage_01_366786438      2  LLMJudge2024\n",
       "4     2002168  msmarco_passage_01_366787276      3  LLMJudge2024\n",
       "...       ...                           ...    ...           ...\n",
       "4418  3100825  msmarco_passage_67_786441513      1  LLMJudge2024\n",
       "4419  3100825  msmarco_passage_67_797018148      1  LLMJudge2024\n",
       "4420  3100825  msmarco_passage_68_250089795      1  LLMJudge2024\n",
       "4421  3100825  msmarco_passage_68_682638740      1  LLMJudge2024\n",
       "4422  3100825  msmarco_passage_68_807682814      2  LLMJudge2024\n",
       "\n",
       "[44230 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_df_list = []\n",
    "\n",
    "for infile in glob.glob(f'./qrels/{results_from}/*.txt'):\n",
    "    judger = infile.split('/')[2].split('.')[0]\n",
    "    result_df = pd.read_csv(infile, sep=' ', header=None, names=['qid', 'Q0', 'docid', 'score'])\n",
    "    result_df.drop(['Q0'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(int)\n",
    "    qrels_df_list.append(result_df)\n",
    " \n",
    "qrels_df = pd.concat(qrels_df_list)\n",
    "qrels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_judged = set(qrels_df['qid'])\n",
    "# real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "# t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "# gpt4_queries_judged = [x for x in queries_judged if x > 3100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/doc_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(qrels_df, qid_to_info, on='qid')\n",
    "data = pd.merge(data, doc_to_info, on='docid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT'] = data['QT'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['QDR'], axis=1, inplace=True)\n",
    "data.drop(['QDS'], axis=1, inplace=True)\n",
    "data.drop(['APL'], axis=1, inplace=True)\n",
    "data.drop(['isGPT4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>score</th>\n",
       "      <th>judged_by</th>\n",
       "      <th>QL</th>\n",
       "      <th>QW</th>\n",
       "      <th>QT</th>\n",
       "      <th>PW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002168</td>\n",
       "      <td>msmarco_passage_00_662986293</td>\n",
       "      <td>3</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47385</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47386</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47387</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47388</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>1</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47389</th>\n",
       "      <td>3100825</td>\n",
       "      <td>msmarco_passage_68_807682814</td>\n",
       "      <td>2</td>\n",
       "      <td>LLMJudge2024</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47390 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid                         docid  score     judged_by  QL  QW QT  \\\n",
       "0      2002168  msmarco_passage_00_662986293      3  LLMJudge2024   0   7  0   \n",
       "1      2002168  msmarco_passage_00_662986293      3  LLMJudge2024   0   7  0   \n",
       "2      2002168  msmarco_passage_00_662986293      2  LLMJudge2024   0   7  0   \n",
       "3      2002168  msmarco_passage_00_662986293      2  LLMJudge2024   0   7  0   \n",
       "4      2002168  msmarco_passage_00_662986293      3  LLMJudge2024   0   7  0   \n",
       "...        ...                           ...    ...           ...  ..  .. ..   \n",
       "47385  3100825  msmarco_passage_68_807682814      2  LLMJudge2024   1  11  2   \n",
       "47386  3100825  msmarco_passage_68_807682814      1  LLMJudge2024   1  11  2   \n",
       "47387  3100825  msmarco_passage_68_807682814      2  LLMJudge2024   1  11  2   \n",
       "47388  3100825  msmarco_passage_68_807682814      1  LLMJudge2024   1  11  2   \n",
       "47389  3100825  msmarco_passage_68_807682814      2  LLMJudge2024   1  11  2   \n",
       "\n",
       "       PW  \n",
       "0      37  \n",
       "1      37  \n",
       "2      37  \n",
       "3      37  \n",
       "4      37  \n",
       "...    ..  \n",
       "47385  47  \n",
       "47386  47  \n",
       "47387  47  \n",
       "47388  47  \n",
       "47389  47  \n",
       "\n",
       "[47390 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling LLM Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_1 = \"label_llm ~ label_human * (QL + QW + PW + QT)\"\n",
    "mixed_model = \"llm_human_diff ~ QL + QW + QT+ PW\"\n",
    "mixed_model_int = \"llm_human_diff ~ QT * (QL + QW + PW)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_1, data, groups=data[\"docid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(result)\n",
    "plot_residuals(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.glm(formula=mixed_model_1, data=data, family=sm.families.Poisson())\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log = data.copy()\n",
    "\n",
    "data_log['label_llm'] = np.log(data_log['label_llm'])\n",
    "model = model = sm.MixedLM.from_formula(mixed_model_1, data, groups=data_log[\"docid\"])\n",
    "\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat = data.copy()\n",
    "\n",
    "data_cat = pd.melt(data_cat, value_vars = ['label_human', 'label_llm'], \n",
    "                   id_vars = ['QL',\n",
    "       'QDR', 'QDS', 'QW', 'PW', 'QT', 'qid', 'docid'],\n",
    "              var_name = 'label_type', value_name = 'label')\n",
    "\n",
    "\n",
    "print(data_cat)\n",
    "\n",
    "mixed_model = \"label ~ label_type * (QW + PW + QT)\"\n",
    "\n",
    "\n",
    "model = sm.MixedLM.from_formula(mixed_model, data_cat, groups=data_cat[\"docid\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cat[['label', 'label_type', 'QW', 'PW', 'QT']]\n",
    "df['label'] = df['label'].astype('category')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a FacetGrid for the label type\n",
    "g = sns.FacetGrid(df, col=\"label_type\", hue=\"QT\", height=5)\n",
    "\n",
    "# Add a histogram to the FacetGrid\n",
    "g.map(sns.histplot, 'label', stat=\"count\", hue = 'QT', multiple='dodge', shrink = .8, common_norm=False, data = df)\n",
    "\n",
    "# Adjust the titles and labels\n",
    "g.add_legend()\n",
    "g.set_axis_labels(\"Relevance Label\", \"Count\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle('Distribution of Relevance Judgements by Label Type and Query Type')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks([0, 1, 2, 3])  # Set y-ticks to 0, 1, 2, 3\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cat[['label', 'label_type', 'QW', 'PW', 'QT']]\n",
    "df['label'] = df['label'].astype('category')\n",
    "df['QT'] = df['QT'].replace({0: 'Human generated Query', 1: 'T5 generated Query', 2: 'GPT-4 generated Query'})\n",
    "df['label_type'] = df['label_type'].replace({'label_human': 'Human Generated Labels', 'label_llm':'GPT-4 Generated Labels'})\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set_style(\"ticks\")  # Options: white, dark, whitegrid, darkgrid, ticks\n",
    "sns.set_context(\"poster\", font_scale =0.6)     # Options: paper, notebook, talk, poster\n",
    "sns.set_palette(\"bright\")   # You can also use: deep, muted, bright, dark, colorblind, or a custom list of colors\n",
    "\n",
    "# Create a FacetGrid for the label type\n",
    "g = sns.FacetGrid(df, col=\"QT\", hue=\"label_type\", height=5)\n",
    "\n",
    "# Add a histogram to the FacetGrid\n",
    "g.map(sns.histplot, 'label', stat=\"proportion\", hue = 'label_type', multiple='dodge', shrink = .8, common_norm=False, data = df)\n",
    "\n",
    "# Adjust the titles and labels\n",
    "g.add_legend()\n",
    "g.set_axis_labels(\"\", \"Proportion\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "titles = ['Relevance Labels for Human Generated Queries', 'Relevance Labels for T5 Generated Queries', 'Relevance Labels for GPT-4 Generated Queries']\n",
    "\n",
    "for ax, title in zip(g.axes.flatten(),titles):\n",
    "    ax.set_title(title)\n",
    "    \n",
    "sns.move_legend(g, ncol=3, loc='upper center', title = '')\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks([0, 1, 2, 3])  # Set y-ticks to 0, 1, 2, 3\n",
    "plt.savefig(\"figs/label_barplots.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "contingency_table = pd.crosstab(df['label'], df['label_type'])\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f'Chi-square statistic: {chi2}, p-value: {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test (one-sided, alternative that GPT-4 labels are greater, null that GPT-4 labels are smaller than or equal to human)\n",
    "stat, p_value = wilcoxon(data['label_human'], data['label_llm'], alternative='less')\n",
    "print(f\"Wilcoxon Signed-Rank statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "stat, p_value = wilcoxon(data['llm_human_diff'], alternative='greater')\n",
    "print(f\"Wilcoxon Signed-Rank statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test (one-sided, alternative that GPT-4 labels are smaller, null that GPT-4 labels are greater than or equal to human)\n",
    "stat, p_value = wilcoxon(data['label_human'], data['label_llm'], alternative='greater')\n",
    "print(f\"Wilcoxon Signed-Rank statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "stat, p_value = wilcoxon(data['llm_human_diff'], alternative='less')\n",
    "print(f\"Wilcoxon Signed-Rank statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test (two-sided)\n",
    "stat, p_value = wilcoxon(data['label_human'], data['label_llm'], alternative='two-sided')\n",
    "print(f\"Wilcoxon Signed-Rank statistic: {stat}, p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(result)\n",
    "plot_residuals(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.glm(formula=mixed_model, data=data_cat, family=sm.families.Poisson())\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.glm(formula=mixed_model, data=data_cat, family=sm.families.NegativeBinomial())\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "\n",
    "\n",
    "# Fit ordinal logistic regression model\n",
    "model = OrderedModel(data_cat['label'], exog, distr='logit')\n",
    "result = model.fit(method='bfgs')\n",
    "\n",
    "# Print the summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_int = \"score ~ isGPT4 + Judge + QL + QDR + QW + C(LLM, Treatment(reference='Other')) + C(Judge, Treatment(reference='nist')) * C(LLM, Treatment(reference='Other')) + isGPT4 * C(LLM, Treatment(reference='Other')) + pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_int, data, groups=data[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_real_judgments = data[((data['Judge'] == 'nist') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(real_queries_judged)))]\n",
    "synthetic_queries_synthetic_judgments = data[((data['Judge'] == 'gpt4') & (data['qid'].isin(t5_queries_judged) | data['qid'].isin(gpt4_queries_judged)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_condition_Qreal = \"score ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\"\n",
    "mixed_model_condition_Qsynthetic = \"score ~ QL + QDS + QW + C(LLM, Treatment(reference='Other')) + pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_real_judgments, groups=real_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_real_judgments, groups=synthetic_queries_real_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qreal, real_queries_synthetic_judgments, groups=real_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_condition_Qsynthetic, synthetic_queries_synthetic_judgments, groups=synthetic_queries_synthetic_judgments[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "real_queries_diff = pd.merge(real_queries_real_judgments, real_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "real_queries_diff['score_ab'] = real_queries_diff['score_a'] - real_queries_diff['score_b']\n",
    "real_queries_diff['score_ba'] = real_queries_diff['score_b'] - real_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", real_queries_diff, groups=real_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "synthetic_queries_diff = pd.merge(synthetic_queries_real_judgments, synthetic_queries_synthetic_judgments[['run_id', 'qid', 'score']], on=['run_id', 'qid'], suffixes=('_a', '_b'))\n",
    "# Subtracting the 'Score' values\n",
    "synthetic_queries_diff['score_ab'] = synthetic_queries_diff['score_a'] - synthetic_queries_diff['score_b']\n",
    "synthetic_queries_diff['score_ba'] = synthetic_queries_diff['score_b'] - synthetic_queries_diff['score_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_queries_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ab ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(\"score_ba ~ QL + QDR + QW + C(LLM, Treatment(reference='Other')) + pipeline\", synthetic_queries_diff, groups=synthetic_queries_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments (refer to: \"Extra Exp. 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_file):\n",
    "    result_df = pd.read_csv(result_file, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_judge_results = get_result(result_file=\"results/all.pass.nist.ndcgeval\")\n",
    "synthetic_judge_results = get_result(result_file=\"results/all.pass.gpt4.ndcgeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes on 'run_id' and 'qid'\n",
    "results_diff = pd.merge(real_judge_results, synthetic_judge_results, on=['run_id', 'qid'], suffixes=('_real', '_synthetic'))\n",
    "# Subtracting the 'Score' values\n",
    "results_diff['score_RS'] = results_diff['score_real'] - results_diff['score_synthetic']\n",
    "results_diff['score_SR'] = results_diff['score_synthetic'] - results_diff['score_real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff = pd.merge(results_diff, qid_to_info, on='qid')\n",
    "results_diff = pd.merge(results_diff, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected: score_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_diff['qd_RS'] = results_diff['QDR'] - results_diff['QDS']\n",
    "results_diff['qd_SR'] = results_diff['QDS'] - results_diff['QDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_1 = \"score_SR ~ Synthetic + QL + qd_SR + QW + DL + pipeline + C(LLM, Treatment(reference='Other')) + Synthetic * C(LLM, Treatment(reference='Other'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.MixedLM.from_formula(mixed_model_1, results_diff, groups=results_diff[\"run_id\"])\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
