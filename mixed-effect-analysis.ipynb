{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43637a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00844e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"map\" # map # ndcg_cut_10\n",
    "result_format = \"treceval\" # treceval # ndcgeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0aa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_list = []\n",
    "\n",
    "for infile in glob.glob(f'./results/TRECDL2023/*.{result_format}'):\n",
    "    judger = infile.split('/')[3].split('.')[2]\n",
    "    result_df = pd.read_csv(infile, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    results_df_list.append(result_df)\n",
    " \n",
    "results_dfs = pd.concat(results_df_list)\n",
    "results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf023a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_judged = set(results_dfs['qid'])\n",
    "real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "gpt4_queries_judged = [x for x in queries_judged if x > 3100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/pass_to_info.txt\", sep='\\t')\n",
    "model_to_info = pd.read_csv(\"infos/model_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d758bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(results_dfs, qid_to_info, on='qid')\n",
    "# data = pd.merge(data, doc_to_info, on='qid')\n",
    "data = pd.merge(data, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='object')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_int = (\n",
    "\"score ~ C(judged_by, Treatment(reference='nist')) * (QDR + QW + DL + isGPT4 + C(ST, Treatment(reference='Other')) + MN) \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int = sm.MixedLM.from_formula(mixed_model_int, data, groups=data[\"run_id\"])\n",
    "result_int = model_int.fit()\n",
    "result_int.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c6527",
   "metadata": {},
   "source": [
    "# NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da28f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'ndcg_cut_10'\n",
    "result_format = 'ndcgeval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_list = []\n",
    "\n",
    "for infile in glob.glob(f'./results/TRECDL2023/*.{result_format}'):\n",
    "    judger = infile.split('/')[3].split('.')[2]\n",
    "    result_df = pd.read_csv(infile, sep='\\t', header=None, names=['run_id', 'metric', 'qid', 'score'])\n",
    "    result_df = result_df[result_df['qid'] != 'all']\n",
    "    result_df['metric'] = result_df['metric'].apply(lambda x: x.rstrip())\n",
    "    result_df = result_df[(result_df['metric'] == metric)]\n",
    "    result_df.drop(['metric'], axis=1, inplace=True)\n",
    "    result_df['judged_by'] = judger\n",
    "    result_df['qid'] = result_df['qid'].astype(int)\n",
    "    result_df['score'] = result_df['score'].astype(float)\n",
    "    results_df_list.append(result_df)\n",
    " \n",
    "results_dfs = pd.concat(results_df_list)\n",
    "results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53484b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_info = pd.read_csv(\"infos/query_to_info.txt\", sep='\\t')\n",
    "doc_to_info = pd.read_csv(\"infos/pass_to_info.txt\", sep='\\t')\n",
    "model_to_info = pd.read_csv(\"infos/model_to_info.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05916997",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_judged = set(results_dfs['qid'])\n",
    "real_queries_judged = [x for x in queries_judged if x < 3000000]\n",
    "t5_queries_judged = [x for x in queries_judged if x > 3000000 and x < 3100000]\n",
    "gpt4_queries_judged = [x for x in queries_judged if x > 3100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(results_dfs, qid_to_info, on='qid')\n",
    "# data = pd.merge(data, doc_to_info, on='qid')\n",
    "data = pd.merge(data, model_to_info, on='run_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_int = (\n",
    "\"score ~ C(judged_by, Treatment(reference='nist')) * (QDR + QW + DL + isGPT4 + C(ST, Treatment(reference='Other')) + MN) \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int = sm.MixedLM.from_formula(mixed_model_int, data, groups=data[\"run_id\"])\n",
    "result_int = model_int.fit()\n",
    "result_int.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
